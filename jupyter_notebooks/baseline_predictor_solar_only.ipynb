{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample code for processing netcdf4 files for kaggle Solar Energy Prediction Competition.\n",
    "https://www.kaggle.com/c/ams-2014-solar-energy-prediction-contest#description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A good link to start wrapping your head around netcdf data format: \n",
    "# https://www.unidata.ucar.edu/software/netcdf/netcdf-4/newdocs/netcdf-tutorial.html#Intro\n",
    "\n",
    "# This is a good link describing the dataset for the competition.\n",
    "# https://www.kaggle.com/c/ams-2014-solar-energy-prediction-contest/discussion/5057\n",
    "\n",
    "# It is important to undertsand that the data provided is the *prediction* of a parameter\n",
    "# (eg. *prediction* of the total precipitation), rather than the *observed* data.\n",
    "# The data is a dictionary of all the helper/axis variables in it once you load the \n",
    "# netcdf4 and the actual data. The actual data is a big array of shape (5113, 11, 5, 9, 16) \n",
    "# with 5113 daily predictions from 1994 to 2007, 11 ensemble members of the GEFS \n",
    "# (different submodel predictions I think), 5 actual predictions (it's released at midnight \n",
    "# I think so it's forcast for 12, 15, 18, 21, and 24 hours out), and 9 latitudes and 16 \n",
    "# longitudes for where the predictions are spatially.\n",
    "# The GEFS is a weather model that just predicts various things at various locations, \n",
    "# and the data is those predictions.\n",
    "\n",
    "# A good python code sample if you prefer a hacker's approach:\n",
    "# http://schubert.atmos.colostate.edu/~cslocum/netcdf_example.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing netcdf4 python library may be non-trivial. The below code is confirmed to work on AWS SageMaker notebook \n",
    "# with 'conda python 3' kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.11\n",
      "  latest version: 4.5.12\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/python3\n",
      "\n",
      "  added / updated specs: \n",
      "    - netcdf4\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    openssl-1.0.2p             |       h14c3975_0         3.5 MB  anaconda\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    ca-certificates: 2018.03.07-0      --> 2018.03.07-0      anaconda\n",
      "    openssl:         1.0.2p-h14c3975_0 --> 1.0.2p-h14c3975_0 anaconda\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "    certifi:         2018.11.29-py36_0 --> 2018.10.15-py36_0 anaconda\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "openssl-1.0.2p       | 3.5 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.11\n",
      "  latest version: 4.5.12\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/python3\n",
      "\n",
      "  added / updated specs: \n",
      "    - s3fs\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    openssl-1.0.2p             |       h14c3975_0         3.5 MB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    ca-certificates: 2018.03.07-0      anaconda --> 2018.03.07-0     \n",
      "    certifi:         2018.10.15-py36_0 anaconda --> 2018.11.29-py36_0\n",
      "    openssl:         1.0.2p-h14c3975_0 anaconda --> 1.0.2p-h14c3975_0\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "openssl-1.0.2p       | 3.5 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -c anaconda netcdf4 --yes\n",
    "!conda install -y s3fs\n",
    "\n",
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest (not necessarily the fastest) way to xfer the data to your SageMaker machine is to:\n",
    "1. download it to your local machine (eg. your laptop)\n",
    "2. upload the file to your AWS S3 bucket (eg: 3://sergey-ML-workshop) \n",
    "3. download the file from AWS S3 bucket to the machine used to host your SageMaker notebook.\n",
    "    3a. in SageMaker Jupyter console, open a terminal window.\n",
    "    3b. in the terminal window, issue a command to copy the file to your data directory. Example:\n",
    "         $cd SageMaker\n",
    "        $cd <YourProjectDirecotory>\n",
    "         $mkdir data\n",
    "        $mkdir data/train\n",
    "         $cd data/train\n",
    "        $aws s3 cp s3://sergey-ML-workshop/data.nc .\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETCDF4\n",
      "odict_keys(['time', 'lat', 'lon', 'ens', 'fhour'])\n",
      "<class 'netCDF4._netCDF4.Dimension'>: name = 'time', size = 5113\n",
      "\n",
      "odict_keys(['time', 'intTime', 'lat', 'lon', 'ens', 'fhour', 'intValidTime', 'Total_precipitation'])\n",
      "[2007122900 2007123000 2007123100]\n",
      "<class 'netCDF4._netCDF4.Dimension'>: name = 'lat', size = 9\n",
      "\n",
      "[31. 32. 33. 34. 35. 36. 37. 38. 39.]\n",
      "<class 'netCDF4._netCDF4.Dimension'>: name = 'lon', size = 16\n",
      "\n",
      "[254. 255. 256. 257. 258. 259. 260. 261. 262. 263. 264. 265. 266. 267.\n",
      " 268. 269.]\n",
      "<class 'netCDF4._netCDF4.Variable'>\n",
      "float32 Total_precipitation(time, ens, fhour, lat, lon)\n",
      "    _FillValue: 9999.0\n",
      "    units: kg m-2\n",
      "    long_name: Total_precipitation_Accumulation (Accumulation for  Mixed Intervals) @ surface\n",
      "    cell_methods: time: sum\n",
      "    GRIB_param_discipline: Meteorological_products\n",
      "    GRIB_param_category: Moisture\n",
      "    GRIB_param_name: Total_precipitation\n",
      "    GRIB_generating_process_type: Forecast\n",
      "    GRIB_param_id: [2 0 1 8]\n",
      "    GRIB_product_definition_template: 8\n",
      "    GRIB_product_definition_template_desc: Average, accumulation, extreme values or other statistically processed value at a horizontal level in a time interval\n",
      "    GRIB_level_type: 1\n",
      "    GRIB_level_type_name: surface\n",
      "    GRIB_interval_stat_type: Accumulation\n",
      "    GRIB_VectorComponentFlag: easterlyNortherlyRelative\n",
      "unlimited dimensions: \n",
      "current shape = (5113, 11, 5, 9, 16)\n",
      "filling on\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset('/home/ec2-user/SageMaker/data/gefs_train/train/apcp_sfc_latlon_subset_19940101_20071231.nc')\n",
    "print (dataset.file_format)\n",
    "print (dataset.dimensions.keys())\n",
    "print (dataset.dimensions['time'])\n",
    "print (dataset.variables.keys())\n",
    "print (dataset.variables['intTime'][5110:])\n",
    "print (dataset.dimensions['lat'])\n",
    "print (dataset.variables['lat'][0:])\n",
    "print (dataset.dimensions['lon'])\n",
    "print (dataset.variables['lon'][0:])\n",
    "print (dataset.variables['Total_precipitation'])\n",
    "print (dataset.variables['Total_precipitation'][0][0][0][0][0])\n",
    "\n",
    "# per https://www.kaggle.com/c/ams-2014-solar-energy-prediction-contest/discussion/5288, \n",
    "# The ensemble control run (the first ensemble member) should have the best fit initial \n",
    "# conditions for each day. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "solar_output = pd.read_csv(\"/home/ec2-user/SageMaker/data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_output[\"Date\"] = pd.to_datetime(solar_output[\"Date\"], format = \"%Y%m%d\")\n",
    "start_date = solar_output[\"Date\"].iloc[0]\n",
    "end_date = solar_output[\"Date\"].iloc[-1]\n",
    "\n",
    "solar_output.index = pd.date_range(start_date, end_date, freq='D')\n",
    "solar_output = solar_output.drop(\"Date\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = solar_output.iloc[:int(solar_output.shape[0]/10*9)]\n",
    "testing = solar_output.iloc[int(solar_output.shape[0]/10*9):]\n",
    "training.to_csv(\"training_split_9_10ths.csv\")\n",
    "testing.to_csv(\"testing_split_9_10ths.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "\n",
    "bucket = \"forcasting-project-group\"\n",
    "prefix = \"forecasting/mvp\"\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "s3_data_path = \"{}/{}/data\".format(bucket, prefix)\n",
    "s3_output_path = \"{}/{}/output\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_training = []\n",
    "for col in training.columns:\n",
    "    time_series_training.append(training[col])\n",
    "    \n",
    "time_series = []\n",
    "for col in testing.columns:\n",
    "    time_series.append(training[col])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = \"utf-8\"\n",
    "freq = 'D'\n",
    "s3filesystem = s3fs.S3FileSystem()\n",
    "\n",
    "def series_to_obj(ts, cat=None):\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": list(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    return obj\n",
    "\n",
    "def series_to_jsonline(ts, cat=None):\n",
    "    return json.dumps(series_to_obj(ts, cat))\n",
    "\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", 'wb') as fp:\n",
    "    for ts in time_series:\n",
    "        fp.write(series_to_jsonline(ts).encode(encoding))\n",
    "        fp.write('\\n'.encode(encoding))\n",
    "\n",
    "with s3filesystem.open(s3_data_path + \"/test/test.json\", 'wb') as fp:\n",
    "    for ts in time_series:\n",
    "        fp.write(series_to_jsonline(ts).encode(encoding))\n",
    "        fp.write('\\n'.encode(encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://forcasting-project-group/forecasting/mvp/validation/testing_split_9_10ths.csv'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_channel = prefix + '/validation'\n",
    "train_channel = prefix + '/train'\n",
    "sess = sagemaker.Session()\n",
    "sess.upload_data(path='/home/ec2-user/SageMaker/Architecting-For-ML-on-Amazon-SageMaker/Starter-Code/training_split_9_10ths.csv', bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path='/home/ec2-user/SageMaker/Architecting-For-ML-on-Amazon-SageMaker/Starter-Code/testing_split_9_10ths.csv', bucket=bucket, key_prefix=validation_channel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "image_name = get_image_uri(boto3.Session().region_name, 'forecasting-deepar')\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c5.18xlarge',\n",
    "    base_job_name='DEMO-deepar',\n",
    "    output_path=\"s3://\" + s3_output_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 3000\n",
    "freq = 'D'\n",
    "prediction_length = len(testing)\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"num_cells\": \"40\",\n",
    "    \"num_layers\": \"3\",\n",
    "    \"likelihood\": \"gaussian\",\n",
    "    \"epochs\": \"20\",\n",
    "    \"mini_batch_size\": \"32\",\n",
    "    \"learning_rate\": \"0.001\",\n",
    "    \"dropout_rate\": \"0.05\",\n",
    "    \"early_stopping_patience\": \"10\"\n",
    "}\n",
    "estimator.set_hyperparameters(**hyperparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: DEMO-deepar-2018-12-20-18-49-08-170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-20 18:49:08 Starting - Starting the training job...\n",
      "2018-12-20 18:49:10 Starting - Launching requested ML instances......\n",
      "2018-12-20 18:50:18 Starting - Preparing the instances for training......\n",
      "2018-12-20 18:51:22 Downloading - Downloading input data...\n",
      "2018-12-20 18:51:51 Training - Training image download completed. Training in progress.\n",
      "\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 INFO 139988546754368] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 INFO 139988546754368] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'dropout_rate': u'0.05', u'learning_rate': u'0.001', u'num_cells': u'40', u'prediction_length': u'512', u'epochs': u'20', u'time_freq': u'D', u'context_length': u'3000', u'num_layers': u'3', u'mini_batch_size': u'32', u'likelihood': u'gaussian', u'early_stopping_patience': u'10'}\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 INFO 139988546754368] Final configuration: {u'dropout_rate': u'0.05', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_layers': u'3', u'epochs': u'20', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'32', u'likelihood': u'gaussian', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'512', u'time_freq': u'D', u'context_length': u'3000', u'_kvstore': u'auto', u'early_stopping_patience': u'10'}\u001b[0m\n",
      "\u001b[31mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 INFO 139988546754368] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 INFO 139988546754368] Using early stopping with patience 10\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 WARNING 139988546754368] `context_length > 500`.  It may be better to reduce the context_length. Consider aggregating the data to a larger frequency (e.g. instead of hours use days)\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 WARNING 139988546754368] `prediction_length > 500`.  It may be better to reduce the prediction_length. Consider aggregating the data to a larger frequency (e.g. instead of hours use days)\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 INFO 139988546754368] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 INFO 139988546754368] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 INFO 139988546754368] Training set statistics:\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 INFO 139988546754368] Integer time series\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 INFO 139988546754368] number of time series: 98\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 INFO 139988546754368] number of observations: 450898\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 INFO 139988546754368] mean target length: 4601\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 INFO 139988546754368] min/mean/max target: 300.0/16647816.9632/39442800.0\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 INFO 139988546754368] mean abs(target): 16647816.9632\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 INFO 139988546754368] contains missing values: no\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 INFO 139988546754368] Small number of time series. Doing 3 number of passes over dataset per epoch.\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 INFO 139988546754368] Test set statistics:\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 INFO 139988546754368] Integer time series\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 INFO 139988546754368] number of time series: 98\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 INFO 139988546754368] number of observations: 450898\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 INFO 139988546754368] mean target length: 4601\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 INFO 139988546754368] min/mean/max target: 300.0/16647816.9632/39442800.0\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 INFO 139988546754368] mean abs(target): 16647816.9632\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:54 INFO 139988546754368] contains missing values: no\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:55 INFO 139988546754368] nvidia-smi took: 0.0252089500427 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:55 INFO 139988546754368] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[12/20/2018 18:51:55 INFO 139988546754368] Create Store: local\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "data_channels = {\n",
    "    \"train\": \"s3://{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"s3://{}/test/\".format(s3_data_path)\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "job_name = estimator.latest_training_job.name\n",
    "\n",
    "endpoint_name = sagemaker_session.endpoint_from_job(\n",
    "    job_name=job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    deployment_image=image_name,\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "\n",
    "    def set_prediction_parameters(self, freq, prediction_length):\n",
    "        \"\"\"Set the time frequency and prediction length parameters. This method **must** be called\n",
    "        before being able to use `predict`.\n",
    "        \n",
    "        Parameters:\n",
    "        freq -- string indicating the time frequency\n",
    "        prediction_length -- integer, number of predicted time points\n",
    "        \n",
    "        Return value: none.\n",
    "        \"\"\"\n",
    "        self.freq = freq\n",
    "        self.prediction_length = prediction_length\n",
    "        \n",
    "    def predict(self, ts, cat=None, encoding=\"utf-8\", num_samples=100, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        Parameters:\n",
    "        ts -- list of `pandas.Series` objects, the time series to predict\n",
    "        cat -- list of integers (default: None)\n",
    "        encoding -- string, encoding to use for the request (default: \"utf-8\")\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_times = [x.index[-1]+1 for x in ts]\n",
    "        req = self.__encode_request(ts, cat, encoding, num_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, prediction_times, encoding)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, encoding, num_samples, quantiles):\n",
    "        instances = [series_to_obj(ts[k], cat[k] if cat else None) for k in range(len(ts))]\n",
    "        configuration = {\"num_samples\": num_samples, \"output_types\": [\"quantiles\"], \"quantiles\": quantiles}\n",
    "        http_request_data = {\"instances\": instances, \"configuration\": configuration}\n",
    "        return json.dumps(http_request_data).encode(encoding)\n",
    "    \n",
    "    def __decode_response(self, response, prediction_times, encoding):\n",
    "        response_data = json.loads(response.decode(encoding))\n",
    "        list_of_df = []\n",
    "        for k in range(len(prediction_times)):\n",
    "            prediction_index = pd.DatetimeIndex(start=prediction_times[k], freq=self.freq, periods=self.prediction_length)\n",
    "            list_of_df.append(pd.DataFrame(data=response_data['predictions'][k]['quantiles'], index=prediction_index))\n",
    "        return list_of_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = DeepARPredictor(\n",
    "    endpoint=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    content_type=\"application/json\"\n",
    ")\n",
    "predictor.set_prediction_parameters(freq, prediction_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_df = predictor.predict(time_series[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_df = predictor.predict(time_series_training[:5])\n",
    "actual_data = time_series[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for k in range(len(list_of_df)):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    actual_data[k].index = pd.to_datetime(actual_data[k].index, format='%Y%m%d')\n",
    "    actual_data[k][-prediction_length-context_length:].plot(label='target')\n",
    "    p10 = list_of_df[k]['0.1']\n",
    "    p90 = list_of_df[k]['0.9']\n",
    "    plt.fill_between(p10.index, p10, p90, color='y', alpha=0.5, label='80% confidence interval')\n",
    "    list_of_df[k].index = pd.to_datetime(list_of_df[k].index, format='%Y%m%d')\n",
    "    list_of_df[k]['0.5'].plot(label='prediction median')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
